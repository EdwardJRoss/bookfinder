{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for SpaCy Textcat\n",
    "\n",
    "We will take the Zero Shot classifications and distill this into a SpaCy textcat classifier.\n",
    "To do this we first need to convert the data into `.spacy` files for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data\n",
    "\n",
    "Load in the data, along zero shot annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero_shot = pd.read_parquet('../data/02_intermediate/zero_shot_contains_book_title_predictions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd\n",
    "    .read_parquet('../data/01_raw/hackernews2021.parquet')\n",
    "    .set_index('id')\n",
    "    .merge(df_zero_shot, left_index=True, right_index=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get clean text\n",
    "\n",
    "Extract text for classification from the HTML title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "CLEAN_PATTERNS = {\n",
    "    re.compile(\"<p>\"): \"\\n\\n\",\n",
    "    re.compile(\"<i>\"): r\"\",\n",
    "    re.compile(\"</i>\"): r\"\",\n",
    "    re.compile('<a href=\"([^\"]*)\"[^>]*>.*?</a>'): r\"\\1\",\n",
    "    re.compile(\"<pre><code>((?:.|\\n)*?)</code></pre>\", flags=re.MULTILINE): r\"\\1\",\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    for match, sub in CLEAN_PATTERNS.items():\n",
    "        s = match.sub(sub, s)\n",
    "    return html.unescape(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = df.assign(clean_text = \n",
    "        lambda _: (\n",
    "            np.where(_.title.isna(), \"\", _.title)\n",
    "            + np.where(~_.title.isna() & ~_.text.isna(), \"<p>\", \"\")\n",
    "            + _.text.fillna(\"\")\n",
    "        ).apply(clean_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to SpaCy\n",
    "\n",
    "Convert the data into a SpaCy format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward/.pyenv/versions/3.10.6/envs/bookfinder/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a Language to represent the doc (it also tokenizes it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an `id` extension to identify which comment it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(\"id\", default=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the rows into Doc objects.\n",
    "Note that using `nlp.pipe` was much faster than `make_doc`.\n",
    "\n",
    "For the classification it expects exactly one correct class so we use `BOOK` and `NOT_BOOK` as the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "def rows_to_tuples(df):\n",
    "    for id, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        is_book = row.prob > threshold\n",
    "        yield (row.clean_text, {\"id\": id, \"cats\": {\"BOOK\": is_book, \"NOT_BOOK\": not is_book}})\n",
    "\n",
    "def tuples_to_docs(tuples):\n",
    "    for doc, context in nlp.pipe(tuples, as_tuples=True):\n",
    "        doc._.id = context['id']\n",
    "        doc.cats = context['cats']\n",
    "        yield doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_train = 0.8\n",
    "\n",
    "df['train'] = np.random.choice([True, False],\n",
    "                                size=len(df),\n",
    "                                p=[prob_train, 1-prob_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just take a random 100_000 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "limit = 100_000\n",
    "sample = df.query('train').sample(n=limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tuples_to_docs(rows_to_tuples(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the DocBin object exhausts the generator, so it can't be used to stream larger-than-memory datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:20<00:00, 710.73it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = DocBin(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.to_disk('../data/02_intermediate/book_zero_shot_train.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the development set too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:23<00:00, 698.33it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = df.query('~train').sample(n=limit)\n",
    "docs = tuples_to_docs(rows_to_tuples(sample))\n",
    "db = DocBin(docs=docs)\n",
    "db.to_disk('../data/02_intermediate/book_zero_shot_dev.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train execute in terminal at project root\n",
    "\n",
    "```\n",
    "spacy train config/spacy_book_classifier.cfg \\\n",
    "    --paths.train data/02_intermediate/book_zero_shot_train.spacy \\\n",
    "    --paths.dev data/02_intermediate/book_zero_shot_dev.spacy \\\n",
    "    --output ./data/06_models/bookcat_zero_shot\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('bookfinder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4322ac7940fe7f685c5063cc32148376694d180d2305a532203ff910a5681b95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
