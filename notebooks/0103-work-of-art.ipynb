{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3e9cbd-2197-40fd-b4ac-453c6a3443bb",
   "metadata": {},
   "source": [
    "# Work of Art Detection\n",
    "\n",
    "Ontonotes contains a Work of Art category, which is described as \"Titles of books, songs, etc.\"\n",
    "Finding Hacker News comments that contain a \"Work of Art\" is a good heuristic for finding comments that may contain the title of a book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be744b1a-b8d4-4b13-a986-f864e0e8ace3",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c4284e-f456-409d-be80-a7619825b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xxhash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e7e2f-95dd-40d5-afc3-16bf9d7d5ecb",
   "metadata": {},
   "source": [
    "Read in all Hacker News Stories from 2021, which [can be downloaded from Kaggle](https://www.kaggle.com/datasets/edwardjross/hackernews-2021-comments-and-stories) (extracted from the BigQuery dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93adeb87-d47d-427c-a265-58363e785cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/01_raw/hackernews2021.parquet').set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69808199-0d22-4afb-8394-a5e360343103",
   "metadata": {},
   "source": [
    "# Split the Data\n",
    "\n",
    "The data will be split deterministically by the by the root story.\n",
    "This allows using features about the comment thread.\n",
    "\n",
    "## Finding the root\n",
    "\n",
    "For each comment the root can be found by walking up the parents recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28185400-acb8-4ecd-a87e-12feac8b27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dict = df['parent'].fillna(df.index.to_series()).to_dict()\n",
    "\n",
    "root_dict = {}\n",
    "\n",
    "for item, parent in parent_dict.items():\n",
    "    while parent in parent_dict:\n",
    "        grandparent = parent_dict[parent]\n",
    "        if parent == grandparent:\n",
    "            break\n",
    "        parent = grandparent\n",
    "    root_dict[item] = parent\n",
    "    \n",
    "df['root'] = df.index.map(root_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58dcf1-371b-44d1-883b-f824fcecbb88",
   "metadata": {},
   "source": [
    "## Deterministic Splitting\n",
    "\n",
    "The hash of the root id with a fixed salt gives a deterministic random split.\n",
    "Choose a 50% training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1bae94-4fb9-46bf-b180-20ab09fdaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(s, salt='hnbooks'):\n",
    "    return xxhash.xxh32_intdigest(str(s)+salt) % 100\n",
    "\n",
    "bucket = df['root'].apply(bucket)\n",
    "\n",
    "df['bucket'] = bucket\n",
    "\n",
    "df['train'] = bucket < 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd7c4-dc9b-4c17-87ef-3cda3d20b111",
   "metadata": {},
   "source": [
    "# Clean the text\n",
    "\n",
    "Hacker News comments have a subset of HTML, remove some of the markup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df85c3d2-0022-4f2c-bde5-b2b4159d1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    text = html.unescape(text)\n",
    "    text = text.replace('<i>', '')\n",
    "    text = text.replace('</i>', '')\n",
    "    text = text.replace('<p>', '\\n\\n')\n",
    "    text = re.sub('<a href=\"(.*?)\".*?>.*?</a>', r'\\1', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50c53d-3484-44a0-9fe0-f69dced1286f",
   "metadata": {},
   "source": [
    "Create a sample of training data with the clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32899d4e-282b-4086-b95c-9b0d0d764697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    df\n",
    "    .query('train & deleted.isna() & dead.isna()')\n",
    "    .rename(columns={'text': 'comment_text'})\n",
    "    .assign(text=lambda _: (\n",
    "        _['title'].fillna('') + '\\n' + _['comment_text'].fillna('')\n",
    "    ).map(clean)\n",
    "           )\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe993f-eaeb-4fc7-b6ce-cfb73464f8cd",
   "metadata": {},
   "source": [
    "# Work of Art Detection\n",
    "\n",
    "We'll use SpaCy's transformer model to extract all the comments that have a `WORK_OF_ART` entity\n",
    "\n",
    "This takes around 6 hours on an RTX 6000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681e9e86-23bb-4768-bf3b-1547adf68caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f645b8-a84e-4f7b-9430-28e83d7b7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557f4ed-4085-47f1-b410-e40dc3c4f0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba549f1b16143f7a8df5d888fdff295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1947961 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_items = sample\n",
    "\n",
    "pred_true = []\n",
    "for idx, doc in tqdm(zip(sample_items.index, nlp.pipe(sample_items.text)),\n",
    "                     total=len(sample_items)):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'WORK_OF_ART':\n",
    "            pred_true.append(idx)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4152015-57ed-4876-8292-6a6d431310d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_true) / len(sample_items), len(pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea8911-5672-497f-9aa7-74509323ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_items['work_of_art'] = False\n",
    "sample_items.loc[pred_true, 'work_of_art'] = True\n",
    "\n",
    "sample_items[['work_of_art']].to_parquet('../data/02_intermediate/work_of_art_predictions.parquet',\n",
    "                                          compression='gzip', engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
